### 第 1 页

#### 摘要

近年来，多模态学习发展迅速。然而，在多模态训练过程中，模型往往倾向于仅依赖一种模态（基于该模态模型能更快学习），从而导致其他模态未被充分利用。现有的平衡训练过程的方法在损失函数、优化器和模态数量方面存在局限性，且仅考虑调制梯度的幅度，而忽略了梯度的方向。为解决这些问题，本文提出了一种新的平衡多模态学习的方法 —— 分类器引导的梯度调制（CGGM），该方法同时考虑梯度的幅度和方向。我们在四个多模态数据集（UPMC-Food 101、CMU-MOSI、IEMOCAP 和 BraTS 2021）上进行了大量实验，涵盖分类、回归和分割任务。结果表明，CGGM 始终优于所有基线方法和其他最先进的方法，证明了其有效性和通用性。我们的代码可在[https://github.com/zrguo/CGGM](https://github.com/zrguo/CGGM)获取。

#### 1. 引言

人类通过多模态感知世界，如视觉、触觉和听觉。这些多模态特征能提供全面的信息，帮助我们理解和探索环境。近年来，多模态学习取得了巨大成功，例如视觉问答 [2]、多模态情感分析 [18] 和多模态检索 [26,13]。

  

尽管多模态学习近年来取得了显著进展，但训练过程中对不同模态信息的利用不足仍是一个挑战。例如，Wu 等人 [25] 提出了 “贪婪学习者假设”，即多模态模型会学习依赖其中一种输入模态（基于该模态能更快学习），而不再继续学习使用其他模态。Huang 等人 [12] 发现，在联合训练过程中，多种模态会相互竞争，部分模态会在竞争中失败。从实验来看，在一些多模态数据集上，仅使用一种模态训练与使用所有模态训练的准确率几乎没有提升 [18,21]。这些理论分析和实验结果表明，多模态学习在充分利用和整合不同模态信息方面效率不高。

### 第 2 页

为解决这一问题，近期研究 [25,17,15,8,28,9] 对多模态学习的训练过程进行了探究，并提出了梯度调制策略，以在某些情况下更好地整合不同模态的信息并平衡训练过程。然而，所有这些方法都因存在局限性而难以轻松应用。例如，Wu 等人 [25]、Peng 等人 [17]、Li 等人 [15] 和 Hua 等人 [11] 提出了基于交叉熵损失的平衡方法，用于分类任务。

  

对于回归任务或其他任务，我们无法使用这些策略。此外，这些方法大多只能处理仅有两种模态的情况。例如，Wu 等人 [25] 提出的条件学习速度，在模态数量超过两种时难以计算和应用。对于模态数量更多的情况，这些方法无法适用。此外，大多数方法仅考虑调制梯度的幅度，而忽略了梯度的方向。

  

基于上述观察，本文提出了一种新的平衡多模态学习的方法 —— 分类器引导的梯度调制（CGGM）。在 CGGM 中，我们考虑更一般的情况，对任务类型、优化器、模态数量等没有限制。此外，我们同时考虑梯度的幅度和方向，以充分促进多模态学习的训练过程。具体而言，我们添加分类器来评估每种模态的利用率，并获取单模态梯度。然后，我们利用利用率自适应地调制编码器梯度的幅度，并使用单模态梯度指导模型朝着更好的方向优化。

  

我们在四个多模态数据集上进行了大量实验：UPMC-Food 101 [23]、CMU-MOSI [27]、IEMOCAP [3] 和 BraTS 2021 [1]。其中，UPMC-Food 101 和 IEMOCAP 是分类任务，CMU-MOSI 是回归任务，BraTS 2021 是分割任务。CGGM 优于所有基线方法和其他最先进的方法，证明了其有效性和通用性。总之，我们的贡献如下：

  

- 提出了 CGGM，通过同时考虑梯度的幅度和方向来平衡多模态学习。
- CGGM 可轻松应用于多种多模态任务和网络，对任务类型、优化器、模态数量等没有限制，具有通用性。我们提出的 CGGM 为各种任务（包括分类、回归和分割任务）带来了持续的改进。大量实验表明，CGGM 优于其他最先进的方法，证明了其有效性。

### 第 3 页

#### 2. 相关工作

多模态学习：多模态学习的主要挑战之一是如何有效利用和整合来自不同模态的信息，以实现互补。根据融合策略，主要有三种多模态融合策略：早期融合、中期融合和晚期融合。在早期融合方法中 [16,24]，来自不同模态的原始数据在输入层面通过拼接或其他方法组合后再输入到模型中。中期融合方法 [14] 在模型架构内的各个中间处理阶段组合来自不同模态的数据。晚期融合方法 [2,18] 通过独立的模型分别处理每个模态的数据，并在后期阶段进行组合。一般来说，晚期融合是多模态学习中主要使用的方法。主要原因 [14] 是多年来每个单模态流的架构都经过精心设计，以实现每种模态的最先进性能。因此，我们可以利用这些预训练模型 [5,6] 来获得更好的结果。因此，在本文中，我们的方法基于晚期融合。

  

这些融合策略能够有效地整合来自不同模态的信息，但在利用不同模态信息实现互补方面的改进有限。换句话说，它们无法处理模态竞争 [12] 或不平衡的多模态学习问题。当主导模态缺失 [10] 或损坏时，性能会显著下降。与这些融合策略不同，我们的方法旨在相对充分地利用每个模态的信息，并解决不平衡的多模态学习问题。

  

平衡多模态学习：充分利用和整合来自多种模态的信息的效率低下，给多模态学习领域带来了巨大挑战。一些研究 [18,21] 表明，仅使用一种模态训练与使用所有模态训练的准确率几乎没有提升。Wang 等人 [22] 表明，使用多种模态的多模态模型甚至可能不如仅使用一种模态的模型。为了平衡多模态学习过程并充分利用不同模态，一系列平衡多模态学习方法被提出 [25,17,15,8,28,9,7,11]。Wu 等人 [25] 提出了条件学习速度，以捕捉模态之间的相对学习速度并平衡学习过程。Peng 等人 [17] 提出了一种梯度调制策略，通过监测每种模态对学习目标贡献的差异来自适应地控制其优化。

### 第 4 页

最近，Fan 等人 [8] 提出了原型模态再平衡策略，为不同模态引入不同的学习策略。Li 等人 [15] 提出了一种自适应梯度调制方法，该方法可以提高具有各种融合策略的多模态模型的性能。Hua 等人 [11] 通过针对与历史模型的竞争进行调和正则化，动态调整学习目标。

  

然而，所有这些先前的工作都有一定的局限性，只能在某些特定情况下使用。例如，Wu 等人 [25] 基于中期融合策略提出了条件学习速度，这使得它难以应用于模态数量超过两种或网络不基于中期融合的情况。Peng 等人 [17]、Fan 等人 [8]、Fu 等人 [9]、Li 等人 [15] 和 Hua 等人 [11] 提出的平衡策略假设使用交叉熵损失，主要用于分类任务。特别是，Peng 等人 [17] 采用了 SGD 优化器。与这些方法不同，我们考虑更一般的情况，对模态数量、优化器、损失函数等没有限制。此外，大多数现有方法只考虑梯度的幅度，而忽略了梯度的方向。相比之下，我们同时考虑了这两者。

#### 3. 提出的方法

3.1 问题设置  
假设有 M 种模态，表示为m1​,m2​,⋯,mM​。我们将多模态数据集表示为D=(xi​,yi​)i=1N​，其中 N 是数据集中的数据数量，xi​=(xim1​​,xim2​​,⋯,ximM​​)。我们考虑多模态模型中最常见的结构（图 1），其中不同模态的输入首先被输入到特定于模态的编码器中，然后所有模态的表示被输入到融合模块中。我们将模态mi​的编码器表示为ϕi​（其中i=1,2,⋯,M），将融合模块表示为 Ω。

  

在正向传播中，特征首先被输入到编码器中：  
hi​=ϕi​(xmi​)(1)  
其中hi​是模态mi​的表示。在获得所有模态的表示后，应用融合模块：  
y^​=F(Ω([h1​,h2​,⋯,hM​]))  
其中y^​是预测结果，[] 是拼接操作，F(⋅)是用于预测答案的预测头。Ω(⋅)融合多模态表示，并输出融合后的特征作为预测标记。


### 第 5 页

#### 3.2 梯度分析

为引入 CGGM，我们首先分析梯度更新过程。我们将损失函数记为L(θ)=N1​∑i=1N​ℓ(y^​θi​,yi)，其中θ表示网络的参数，y^​θi​是预测值，yi是真实值。为简单起见，在下面的内容中，我们用y^​i表示预测值。与之前仅考虑交叉熵损失的方法 [17,8,11] 不同，我们的损失函数可以是交叉熵损失、L1 损失或其他任何损失函数。使用梯度下降（GD）优化方法，融合模块Ω和编码器ϕi​的参数可以更新为：

  

θt+1Ω​=θtΩ​−α∇θΩ​L(θtΩ​)=θtΩ​−αN1​∑n=1N​(∂Ω∂F​)⊤∂F∂ℓ(y^​n,yn)​

  

θt+1ϕi​​=θtϕi​​−α∇θϕi​​L(θtϕi​​)=θtϕi​​−αN1​∑n=1N​(∂Ω∂F​∂ϕi​∂Ω​)⊤∂F∂ℓ(y^​n,yn)​

  

其中α是学习率，N是批大小，t是迭代次数。根据反向传播中用于求梯度的链式法则，ϕi​的更新会影响Ω的输入，反之亦然。从图 2（a）和 2（b）可以看出，主导模态的梯度和准确率在训练过程中会上升，而其他两种模态则保持稳定。特别是文本模态的梯度幅度在训练过程中增长很快。这表明主导模态的编码器更新速度会比其他模态快得多，这使得∂ϕ∂Ω​大得多，这一现象也得到了先前工作 [8,17,25] 的证实。此外，在图 2（c）中，我们展示了每个模态与融合之间的梯度方向。可以观察到，音频模态与多模态融合之间的相似度小于 0，表明它们朝着相反的方向优化，从而阻碍了多模态分支的梯度更新。同时，文本模态与多模态融合之间的相似度在增加，表明优化方向朝着主导模态。随着优化的进行，主导模态的编码器可以做出相对准确的预测，这使得融合模块Ω仅依赖于该模态（如前面提到的幅度和方向），而其他编码器则优化不足。

#### 3.3 分类器引导的梯度调制

3.3.1 梯度幅度调制  
正如我们在 3.2 节中讨论的，主导模态的梯度幅度在训练过程中增长迅速，而其他模态则保持稳定，因此优化不足。为了平衡训练过程并使融合模块同时受益于所有编码器，我们提出了分类器引导的梯度调制。具体来说，我们使用特定于模态的分类器对式（1）中的hi​进行预测。我们可以将这个过程写为：

  

y^​mi​​=fi​(hi​)(5)

  

其中fi​是模态mi​的分类器，y^​mi​​是仅使用模态mi​的预测。分类器fi​由 1-2 个多头自注意力（MSA）层 [20] 和一个全连接层组成，用于分类和回归任务。对于分割任务，fi​是一个轻量级解码器。在hi​被输入到融合模块Ω之后，它变成了一个更高层次的表示。因此，我们使用几个 MSA 层使hi​与融合模块的输出更加一致。

### 第 6 页

对于特定任务，我们有一些评估指标，如准确率和平均绝对误差。在这里，我们选择其中一个评估指标（例如分类任务的准确率和回归任务的平均绝对误差）并将其记为ε。在训练的每次迭代中，我们可以从分类器中获得预测。我们将这些预测记为y^​i=(y^​m1​i​,y^​m2​i​,⋯,y^​mM​i​)，其中i是当前迭代。此外，我们使用y^​i评估任务，以获得评估指标εi=(εm1​i​,εm2​i​,⋯,εmM​i​)。在这里，我们使用两次连续ε之间的差异来表示每次迭代的特定于模态的改进：

  

Δεt+1​=εt+1−εt=(Δεm1​t+1​,Δεm2​t+1​,⋯,ΔεmM​t+1​)=(εm1​t+1​−εm1​t​,εm2​t+1​−εm2​t​,⋯,εmM​t+1​−εmM​t​)​(6)

  

其中t=0,1,2,⋯,T，T是训练的总迭代次数。特别地，ε0被初始化为 0。在一些多模态数据集中，仅使用其中一种模态就可以取得良好的结果，因此我们不能直接使用ε来衡量不同模态的利用率。因此，使用ε之间的差异来表示每次迭代的相对改进是合理的。然后，我们定义第t次迭代中模态mi​的梯度幅度平衡项如下：

  

Bmi​t​=ρ∑k=1M​Δεmk​t​∑k=1,k=iM​Δεmk​t​​(7)

  

其中ρ是缩放超参数，M是模态的数量。根据式（7），很容易发现，当仅使用模态mi​的模型性能改进非常快时（即Δεmi​t​很大），Bmi​t​会很小。类似地，当模态mi​给模型带来的改进相对有限时（即Δεmi​t​很小），Bmi​t​会很大。因此，Bmi​t​能够衡量这些模态的相对利用率，我们可以使用Bmi​t​来调制编码器ϕi​的梯度幅度。因此，式（4）可以重写为：

  

θt+1ϕi​​​=θtϕi​​−αBmi​t+1​∇θϕi​​L(θtϕi​​)=θtϕi​​−αρ∑k=1M​Δεmk​t+1​∑k=1,k=iM​Δεmk​t+1​​∇θϕi​​L(θtϕi​​)​

  

根据式（2），我们知道最终预测与hi​密切相关。因此，在我们调制相应编码器ϕi​的梯度后，它会对Ω的输入产生影响，进而有助于融合模块Ω的优化。

  

3.3.2 梯度方向调制  
正如 Wu 等人 [25] 所发现的，当模型仅依赖一种模态就能表现良好时，它不会继续学习使用其他模态。如 3.2 节所讨论的，这意味着该模态主导了模型的更新。先前的工作 [25,17,15] 主要通过关注梯度幅度调制来解决这个问题。然而，在 3.2 节中，我们发现模型是朝着主导模态优化的。因此，在本小节中，我们介绍一种可以调制梯度方向以平衡训练过程的方法。

  

一般来说，当模型仅依赖一种模态进行预测时，我们希望平衡模型的优化方向。因此，我们建议强制模型的梯度方向尽可能接近仅使用一种模态的模型的加权平均梯度方向。我们使用式（7）中的Bmi​t​作为权重项。这确保了当模型倾向于朝着主导模态优化时，Bmi​t​可以帮助模型使用来自其他模态的信息。此外，由于Bmi​t​在训练过程中会发生变化，该项可以进行动态调整以平衡优化方向。具体来说，我们可以在训练过程中向模型输入一种模态，并通过将其他模态替换为 0 或其他固定值来丢弃它们，以计算该模态的梯度。通过这种方法，我们可以计算所有模态的单模态梯度。然后，我们只需强制模型的梯度方向尽可能接近这些单模态梯度方向的加权平均值。然而，这种方法在训练过程中非常复杂。


### 第 7 页

因为在每次迭代中，我们都需要丢弃模态来计算单模态梯度，随着模态数量的增加，这会非常耗时。

  

因此，我们建议使用分类器的梯度来表示单模态梯度。稍后我们将证明它们是相似的（4.4 节和图 4）。这里，我们以回归任务的梯度为例，其输出维度为 1，因此梯度是一个 n 维向量。对于分类任务或其他梯度为矩阵的任务，详见附录 A。具体来说，我们可以计算分类器fi​的梯度为：

  

∇θfi​​L(θfi​)=∂fi​∂L(θfi​)​=[∂θ1fi​​∂L(θfi​)​,∂θ2fi​​∂L(θfi​)​,⋯,∂θnfi​​∂L(θfi​)​]⊤

  

其中θfi​表示fi​的参数。与式（3）和式（4）中t表示迭代次数不同，这里的θfi​表示fi​的参数变量之一。类似地，我们可以计算融合模块分类器F的梯度为：

  

∇θF​L(θF)=∂F∂L(θF)​=[∂θ1F​∂L(θF)​,∂θ2F​∂L(θF)​,⋯,∂θnF​∂L(θF)​]⊤

  

我们将∇θfi​​L视为单模态梯度，将∇θF​L视为融合梯度。如前所述，我们希望∇θF​L尽可能接近∇θfi​​L的加权平均方向。令sim(u,v)表示归一化后的u和v的点积（即余弦相似度）。我们可以通过最大化它们的余弦相似度，强制融合模块的梯度方向尽可能接近这些单模态梯度方向的加权平均值：

  

max∑i=1M​Bmi​t​sim(∇θF​L,∇θfi​​L)

  

其中t是当前迭代次数。我们将式（11）重写为损失项：

  

Lgmt​=M1​∑i=1M​​Bmi​t​​−Bmi​t​sim(∇θtF​​L,∇θtfi​​​L)

  

余弦相似度是一个介于 - 1 和 1 之间的数。通过在损失项中加入∣Bmi​t​∣，我们可以确保损失Lgm​始终为正。如前所述，当模态mi​的改进有限时，Bmi​t​较大。因此，Lgmt​中的相应项会较大，使优化方向朝向模态mi​，从而平衡学习过程。

  

然后，我们的总损失函数可以写为：

  

Lt=Ltask​+λLgmt​(13)

  

其中Ltask​是任务损失函数（例如交叉熵损失、L1 损失），λ是两个损失项之间的权衡参数。我们在算法 1 中展示了我们的整体方法。

### 第 8 页

#### 4. 实验

4.1 数据集和评估指标  
我们使用了四个多模态数据集：UPMC-Food 101 [23]、CMU-MOSI [27]、IEMOCAP [3] 和 BraTS 2021 [1]。表 1 展示了这四个数据集的差异。

  

UPMC-Food 101 [23] 是一个食品分类数据集，包含约 100,000 个食谱，涉及 101 个食品类别。数据集中的每个条目由一张图像和文本信息表示。我们使用准确率和 F1 分数来评估模型的性能。

  

CMU-MOSI [27] 是一个流行的多模态（音频、文本和视频）情感分析数据集。每个视频片段都被人工标注了从强烈负面到强烈正面（-3 到 + 3）的情感分数。遵循先前的工作 [18,10]，我们使用二元准确率（ACC-2）、F1 分数、7 类准确率（ACC-7）、平均绝对误差（MAE）和皮尔逊相关系数（Corr）来评估模型的性能。

  

IEMOCAP [3] 是一个多模态情感识别数据集，包含来自十个演员在五个双人对话会话中的录制视频。遵循先前的工作 [18,24,10]，我们选择了四种情感（快乐、愤怒、悲伤和中性状态）进行情感识别。我们使用准确率和 F1 分数来评估模型的性能。

  

BraTS 2021 [1] 是一个 3D 多模态脑肿瘤分割数据集，具有四种模态：flair、t1ce、t1 和 t2。输入图像大小为 240×240×155。标注被组合成三个嵌套的子区域：全肿瘤（WT）、肿瘤核心（TC）和增强肿瘤（ET）。我们使用这三个嵌套子区域的 Dice 分数及其平均值来进行评估。

  

4.2 实现细节  
输入：对于 UPMC-Food 101，我们使用提取的特征作为输入。具体来说，我们使用预训练的 bert-base-uncased 模型 [5] 提取文本特征，并使用在 ImageNet 上预训练的 ViT [6] 提取图像特征。对于 CMU-MOSI 和 IEMOCAP，我们遵循 Guo 等人 [10] 的方法提取声学、视觉和文本特征。对于 BraTS 2021，我们使用预处理后的原始图像作为输入。

  

backbone：对于 UPMC-Food 101、CMU-MOSI 和 IEMOCAP，我们使用 Transformer 编码器 [20] 作为模态编码器和融合模块。对于 BraTS 2021 数据集，我们使用 DeepLab v3+[4] 作为编码器，并使用几个卷积层作为融合模块。

  

训练细节：对于 UPMC-Food 101 和 BraTS 2021 中的图像，我们实施了数据增强策略，包括随机裁剪、随机翻转、颜色抖动、添加噪声等。为了节省内存，我们通过从 3D 图像中随机切片，将 BraTS 2021 视为 2D 分割任务。对于 CMU-MOSI，我们使用 L1 损失作为损失函数。对于 UPMC-Food 101 和 IEMOCAP，我们使用交叉熵损失。对于 BraTS 2021，我们使用软 Dice 损失和交叉熵损失的组合。此外，对于 CMU-MOSI，我们使用 Adam 优化器；对于 UPMC-Food 101 和 IEMOCAP，使用 AdamW 优化器；对于 BraTS 2021，使用 SGD 优化器。其他超参数在附录 B 中有详细描述。

  

4.3 主要结果  
与最先进方法的比较：我们将我们的 CGGM 与其他方法进行比较，以证明我们提出的方法的有效性。对于这四个数据集，我们将 CGGM 与仅使用一种模态训练的模型、多模态联合训练（基线）、模态随机丢弃（MRD）和特定模态学习率（MSLR）方法进行了比较。此外，我们还将 CGGM 与最先进的方法进行了比较，包括 G-Blending [22]、Greedy [25]、OGM [17]、AGM [15]、PMR [8]、UMT [7]、UME [7]、QMF [28] 和 ReconBoost [11]。表 2、表 3 和表 4 展示了 CGGM 及其对比方法的结果。与基线方法相比，CGGM 显著提高了模型的性能，这证明了我们提出的方法的有效性。此外，CGGM 同时考虑了梯度的幅度和方向，因此在所有四个数据集上始终优于其他梯度调制方法。最重要的是，CGGM 可以轻松应用于各种任务，并且具有良好的性能，包括分类任务、回归任务、分割任务等。同时，CGGM 对优化器、损失函数和模态数量没有限制。

  

CGGM 的有效性：在图 3 中，我们可视化了使用 CGGM 训练期间准确率、梯度幅度和方向的变化。与图 2（a）相比，图 3（a）中文本模态的准确率在使用 CGGM 时没有增长得很快，这表明 CGGM 在优化过程中对主导模态施加了约束。此外，所有模态和融合的准确率都有所提高，表明了 CGGM 的有效性。此外，在图 2（b）中，主导模态始终具有最大的梯度，而在图 3（b）中，文本模态的梯度幅度起初有所下降，这表明 CGGM 减慢了其优化速度并加速了其他模态的优化，帮助每个模态充分学习，从而提高了多模态性能。在梯度方向方面，在图 2 中，音频模态与融合之间的相似度在训练过程中始终小于 0，表明单模态和多模态之间的优化方向相反，从而阻碍了优化过程。在图 3 中，我们观察到多模态方向与所有模态一致，表明多模态分支有效地利用了单模态信息。


### 第 9 页

4.4 分类器性能和梯度方向  
分类器性能：在表 5 中，我们展示了不同情况下分类器的准确率。单模态训练可以被视为充分利用单模态信息的基线。与单模态训练相比，多模态训练中f1​和f2​的准确率略有下降，而f3​的准确率略有上升。这表明多模态训练不能充分利用音频和视频的信息，即它们未被充分优化。f3​的提升表明文本模态被充分利用，并且从其他两种模态中学习到了一些信息。相比之下，CGGM 中三个分类器的准确率都有所提升。这表明在平衡过程中，融合能够充分利用所有模态的信息，进而使三种模态的编码器在反向传播过程中融合来自其他模态的信息。因此，三个分类器的准确率都相应提高。这也验证了 CGGM 的有效性。

  

分类器梯度方向：在 3.3.2 节中，我们提出使用分类器的梯度来表示单模态梯度。在本小节中，我们对分类器的梯度和单模态梯度进行了可视化。具体来说，对于每一批数据，我们将它们输入到模型中以获得表示hi​，然后将其输入到分类器fi​中以获得分类器的梯度。然后，我们仅将一种模态的hi​输入到融合模块Ω中以获得单模态梯度。我们使用 t-SNE [19] 对梯度向量进行可视化。图 4 显示了四个数据集上的可视化结果。如图所示，对于每种模态，单模态梯度向量和相应分类器的梯度向量非常接近，这表明使用分类器的梯度来表示单模态梯度是有意义的。

### 第 10 页

4.5 消融研究  
梯度幅度和方向：为了分别衡量梯度幅度调制和梯度方向调制的贡献，我们在 IEMOCAP 上展示了消融实验结果，如表 6 所示。与第一行的基线相比，调制梯度幅度比调制梯度方向给模型性能带来的提升更大。与表 4 中的 CGGM 性能相比，结合梯度幅度和方向调制进一步提升了模型性能。

  

缩放超参数ρ：为了探究缩放超参数ρ对模型性能的影响，我们选择了七个不同的ρ值，并在 IEMOCAP 上展示了结果，如图 5（a）所示。我们发现，在ρ=1.2时达到最高值之前，准确率随着ρ的增加而提高。之后，准确率随着ρ的增加而下降。与基线相比，无论ρ取多大，调制梯度幅度都能带来持续的提升。直观地说，ρ越大，对梯度的修改就越大。因此，表中的结果表明我们需要谨慎选择ρ，以避免修改过大或过小。

  

损失权衡参数λ：λ衡量了我们调制梯度方向的强度。我们选择了六个不同的λ值，并在 IEMOCAP 上展示了结果，如图 5（b）所示。如图所示，当λ为 0.01 或 0.25 时，准确率会略有下降。当λ过小时，调制不足，可能会影响优化过程。当λ过大时，调制过大，会影响任务损失，从而使优化偏离任务目标。

#### 5. 结论

在本文中，我们提出了 CGGM，一种新的平衡多模态训练过程的策略。与现有的梯度调制方法相比，CGGM 在损失函数、优化器、模态数量等方面没有限制。此外，我们在分类器的指导下同时考虑了梯度的幅度和方向。大量的实验和消融研究充分证明了 CGGM 的有效性和通用性。然而，CGGM 也有一个局限性。CGGM 需要利用额外的分类器来实现梯度调制。尽管这些分类器是轻量级的，但它们仍然会消耗更多的计算资源。我们将这个具有挑战性的问题留作未来的工作。

#### 致谢

本工作得到国家重点研发计划（项目编号：2022ZD0162000）的支持。